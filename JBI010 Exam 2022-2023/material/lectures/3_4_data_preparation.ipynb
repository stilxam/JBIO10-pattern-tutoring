{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_(c) 2022, Mark van den Brand and Lina Ochoa Venegas, Eindhoven University of Technology_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [1. Introduction](#introduction) \n",
    "* [2. Data Science](#data-science)\n",
    "* [3. Data Prepration](#data-prep)\n",
    "* [4. Data Access](#access)  \n",
    "* [5. Data Cleanse](#cleanse)  \n",
    "* [6. Data Integration](#integration)\n",
    "* [7. Data Transformation](#transformation)\n",
    "* [8. The Airbnb Case](#airbnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction <a class=\"anchor\" id=\"introduction\"></a>\n",
    "\n",
    "This chapter introduced *data science* and the main aspects we need to consider when performing *data preparation*.\n",
    "Data preparation is one of the main steps we need to follow when performing a data analysis project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Data Science <a class=\"anchor\" id=\"data-science\"></a>\n",
    "Statistics is fundamental for data science; its use is orthogonal to any data science problem. But what is data science in the end? **Data science** is an interdisciplinary field that combines *statistics*, *computer science*, and *domain knowledge* methods to extract knowledge from data to solve problems in a given domain. In this sense, \n",
    "\n",
    "* *statistics* are used to model and analyze datasets; \n",
    "* *computer science* is used to design algorithms that help to process, store, and apply statistical methods to the data at hand, and;\n",
    "* *domain knowledge* is used to define a clear problem and provide a concrete answer to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>Data science</b><br>\n",
    "    <p>Data science is an interdisciplinary field that combines statistics, computer science, and domain knowledge methods to extract knowledge from data to solve problems in a given domain.<p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation <a class=\"anchor\" id=\"data-prep\"></a>\n",
    "\n",
    "**Data preparation** is a critical stage within the data analysis process given that data should be structure in such a way that it eases the extraction of information and does not alter the expected results. \n",
    "It is required because there might be missing values, missing variables<sup>1</sup> that must be computed based on existing features, discrepancies in the levels of categorical variables, wrong data formats, erroneous observations, to name but a few. \n",
    "\n",
    "When conducting a data analysis project, we consider the following stages:\n",
    "\n",
    "1. **Data preparation:** raw data is manipulated and transformed so further analysis can be performed atop of it to solve a domain problem.\n",
    "2. **Exploratory data analysis:** assumptions are verified, methods are chosen, and quality hypotheses are formulated.\n",
    "3. **Confirmatory data analysis:** (statistical hypothesis testing) previously defined hypotheses are tested and conclusions are drawn from the analysis.\n",
    "\n",
    "Although we placed *data preparation* as the first stage of the data analysis process, it is common that after performing the exploratory data analysis, data scientists go back to the underlying data and prepare it even further. \n",
    "This is natural after identifying irrelevant or erroneous observations, as well as errors in the formatting of the values.\n",
    "\n",
    "<sup>1</sup> In the context of data science a *variable* models a feature of a case or observation. \n",
    "They are represented as columns in a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>Data preparation</b><br>\n",
    "    <p>Data preparation is a stage in the data analysis process where raw data is manipulated and transformed to support further analysis targeting a domain problem.<p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation comprises the following subprocess:\n",
    "\n",
    "* **Data access:** accessing and discovering the dataset.\n",
    "* **Data cleanse:** cleaning the data by treating faulty and inconsistent data.\n",
    "* **Data integration:** merging or joining multiple data sources together.\n",
    "* **Data transformation:** normalizing, enriching, generalizing, or reducing the data.\n",
    "\n",
    "The application of each subprocess in a dataset will depend on the nature of the data and the domain problem to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Access <a class=\"anchor\" id=\"access\"></a>\n",
    "\n",
    "The data preparation stage begins with accessing and discovering the data. Evaluating if the data is correct and enough is part of the duties of a good data scientist. It must be clear:\n",
    "\n",
    "* what was the **methodology** used to extract the data;\n",
    "* which **data structure** should be used to represent the data;\n",
    "* which **format** is used to store the data, and;\n",
    "* which **variables** or **features** are available. \n",
    "\n",
    "This information will be useful to assess and understand if the data at hand is enough to solve the domain problem, and if not, what additional data is needed. The result of this process will be finding a new dataset to replace or complement the existing one (c.f. [Data Integration](#integration)) or adding new variables to the existing dataset (c.f. [Data Transformation](#trannsformation)). \n",
    "\n",
    "For this book, we will only consider *rectangular data* (two-dimensional matrix or table), but there are many other data structures used in the wild."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>Data access</b><br>\n",
    "    <p>Subprocess within the data analysis stage that focuses on accessing and discovering the data.<p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Cleanse <a class=\"anchor\" id=\"cleanse\"></a>\n",
    "\n",
    "The **data cleanse** stage is one of the most important parts of the data preparation process, mainly because is at this moment where faulty data and inconsistencies should be treated. \n",
    "If done incorrectly, the subsequent analysis might result in biased or inaccurate results. \n",
    "Some of the main tasks that should be carried on during the data cleanse stage include:\n",
    "\n",
    "* removing **erroneous data**,\n",
    "* dealing with **missing values**,\n",
    "* **formatting values**, and\n",
    "* masking **sensitive data**.\n",
    "\n",
    "How to deal with these and other data issues will depend on the data source you are using. \n",
    "There is no rule of thumb to follow for all cases. \n",
    "However, you can always be conscious about the previously mentioned tasks, and evaluate how to align them with your data. \n",
    "Hereafter, we present the main points to consider when cleaning a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>Data cleanse</b><br>\n",
    "    <p>Subprocess within the data analysis stage that focuses on cleaning the data by treating faulty and inconsistent data.<p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erroneous Data\n",
    "\n",
    "When accessing your data, you might encounter noisy or wrong values, duplicated records, or even irrelevant observations.\n",
    "\n",
    "* **Noisy or wrong values** can be detected by checking that all values of a given variable are contained within the accepted *domain* of that variable. \n",
    "If not, you need to isolate and analyze these cases. \n",
    "Which values do they have? \n",
    "Do you need to redefine the domain of the variable? \n",
    "Or on the contrary, do you need to remove the record because the data is erroneous? \n",
    "Can you still keep the record and replace the faulty value with a default value, so you do not lose information relevant to other variables? \n",
    "All these questions and many more must be considered when deciding how to deal with noisy or wrong values in your data. \n",
    "Additionally, some outliers might be placed in this category. \n",
    "But beware! Not all outliers are faulty data. You need to further analyze them to draw a valid conclusion.\n",
    "\n",
    "* **Duplicated records** (or data redundancy) are also a point of discussion during the data cleanse stage. \n",
    "Do you want to keep them or you must get rid of them? The answer–once again–will depend on the problem at hand. \n",
    "Sometimes keeping duplicated records is required because they show the frequency of an event that is relevant to solve the domain problem. \n",
    "However, oftentimes, these duplicates have been wrongly included in the dataset and should be removed to avoid getting biased results.\n",
    "\n",
    "* **Irrelevant observations** might also be included in your data. \n",
    "It might happen that you are using a dataset that explores a universe that is not part of the problem you want to solve. \n",
    "For instance, you might have a dataset that reports observations of a given domain for all the continents, but it might be the case that your problem focuses on only one of them. \n",
    "Then, having these extra cases will steer your research in the wrong direction.\n",
    "To avoid these issues, you must get rid of these irrelevant observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values\n",
    "\n",
    "Information loss and dropouts or nonresponses are typical causes of missing values. \n",
    "The former usually happens as a consequence of varying accuracy and precision of the different data sources or devices used to extract the dataset; the latter appears when conducting studies that involve surveys or interviews. \n",
    "It is not rare to face cases where certain values are missed in the process of measuring and formatting data. \n",
    "It is up to you as a data scientist to decide how to deal with these cases. \n",
    "The options are pretty similar to the ones you have when dealing with noisy or wrong values; you can decide to ignore these observations and remove them from the dataset or replace them with a default value. \n",
    "If you opt for the latter, you need to carefully decide which is the default value you will be using, is it the mean, the median, a prediction, or a flag (like `-1` or `NaN`) that indicates that the value is missing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Formats\n",
    "\n",
    "It is not rare to face observations containing the right data but the wrong format. \n",
    "This is particularly frequent when dealing with dates or timestamps: the format might be different among different records. \n",
    "To properly process this data you will need to define a standard and modify all values accordingly. \n",
    "Each variable must have a specific format, and all values within that variable should comply with it. \n",
    "Be as meticulous as possible when dealing with these cases; even forgetting about capitalizing a word or adding an extra space that does not comply with your format will make you lose a lot of time and effort! \n",
    "This is specially important when dealing with dates:\n",
    "choosing the right time zone is essential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitive Data\n",
    "\n",
    "As a data scientist, you have a big responsibility when dealing with private data. \n",
    "During the last decades, we have witnessed the rise of international regulations–such as the General Data Protection Regulation ([GDPR](https://gdpr.eu/what-is-gdpr/))–that enforces certain security practices to protect the rights and privacy of people involved in data gathering studies.\n",
    "Therefore, sensitive data should be masked in your dataset. \n",
    "**Data masking** is the process of hiding data in the original dataset. \n",
    "It can be performed using **pseudonymization** or **anonymization** (Tachepun, 2020). \n",
    "In the former, data is kept private by transforming it into another form. \n",
    "Some of the techniques used to pseudonymize data are:\n",
    "\n",
    "* **Encryption:** plain text data is encoded in ciphertext format. \n",
    "To do so a key and algorithm are used. \n",
    "* **Tokenization:** data is transformed but keeps its uniqueness–that is, the same values will be mapped to the same alternative representation. \n",
    "For instance, every value `'Omar'` will be mapped to the same value (let us say) `'Rolt'`.\n",
    "* **Scrambling:** data obfuscation is permanent and there is no way to go back to the original value. \n",
    "\n",
    "In the case of anonymization, we can use the following techniques:\n",
    "\n",
    "* **Suppression:** certain characters are replaced by a special symbol such as `*`. \n",
    "For instance, a credit card might be represented as `'**** **** **** 2789'`.\n",
    "* **Generalization:** values are discretized–that is, they are replaced by the range in which they are located. \n",
    "For instance, the age `23` can be replaced by `'20-25'`.\n",
    "\n",
    "Although extremely important, masking sensitive data is out of the scope of this book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Integration <a class=\"anchor\" id=\"integration\"></a>\n",
    "\n",
    "The **data integration** stage is required when dealing with multiple data sources. \n",
    "These independent data sources can be merged or joined together. \n",
    "The main issues faced during this stage involve dealing with:\n",
    "\n",
    "* **Schema integration:** you should be able to merge the schemas of the different data sources into one schema. \n",
    "\n",
    "* **Data conflicts:** conflicts can be related to *naming conflicts*, either by introducing synonyms (two variables with different names representing the same feature) or homonyms (two variables with the same name represent different features); *type conflicts*, where the same variable is modeled with a different data type in two sources, and; *domain conflicts*, where the same variable has a different domain in two sources, among others.\n",
    "\n",
    "* **Data redundancy:** merging multiple data sources might result in duplicated records or variables. \n",
    "In the former, the duplicated record might be expected if it is describing a different observation with the same attributes. \n",
    "However, the rationale behind keeping or removing a record must be clear. \n",
    "Having redundant information might negatively impact the readability and complexity of your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>Data integration</b><br>\n",
    "    <p>Subprocess within the data analysis stage that focuses on merging or joining multiple data sources.<p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Transformation <a class=\"anchor\" id=\"transformation\"></a>\n",
    "\n",
    "The **data transformation** stage involves modifying the existing data to *normalize*, *enrich*, *generalize*, or *reduce* it.\n",
    "\n",
    "* **Data normalization:** mapping values in the original range of a variable into an output range. Beware that depending on the chosen method distortion or bias might be introduced in the data.\n",
    "* **Data enrichment:** data from different variables and sources are added or related resulting in new variables in the dataset. One type of data enrichment is *data aggregation*, which results from the operation of aggregating or summarizing multiple values in one. \n",
    "* **Data generalization:** values of a variable are replaced with high-level factors or ranges. This is useful to perform analysis based on categories.\n",
    "* **Data reduction:** data is reduced to ease its manipulation and analysis. Irrelevant variables might be removed. In addition, the number of observations can also be reduced utilizing different techniques such as random sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>Data transformation</b><br>\n",
    "    <p>Subprocess within the data analysis stage that normalizes, enriches, generalizes, or reduces the data at hand.<p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. The Airbnb Case <a class=\"anchor\" id=\"airbnb\"></a>\n",
    "\n",
    "<img src=\"assets/airbnb.jpeg\" alt=\"Airbnb\" width=\"500\"/>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <span style=\"font-size:0.9em; font-weight: bold;\"><b>The Airbnb case.</b></span>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "To apply the concepts we have learned in this notebook, we will use an adapted version of the *Airbnb Listings & Reviews* available [here](https://www.kaggle.com/mysarahmadbhat/airbnb-listings-reviews) under a CC0 1.0 Universal license. The dataset contains 279,712 listings of Airbnb in 10 major cities and more than 1 million reviews. The dataset comes in two different CSV files: the `listing.csv` and the `reviews.csv` files. The former contains the following variables.\n",
    "\n",
    "| Variable   | Description |\n",
    "|------------|-------------|\n",
    "| `listing_id`           | ID of the listing in Airbnb |\n",
    "| `name`                 | Name of the listing |\n",
    "| `neighbourhood`        | Neighbourhood where the listing is located |\n",
    "| `district`             | District where the listing is located |\n",
    "| `city`                 | City where the listing is located |\n",
    "| `property_type`        | Type of the property (e.g. entire apartment, private room) |\n",
    "| `room_type`            | Type of (e.g. entire place, hotel room) |\n",
    "| `accommodates`         | Maximum number of guests |\n",
    "| `amenities`            | Amenities of the listing |\n",
    "| `price`                | Price of the listing in local currency |\n",
    "| `review_scores_rating` | Listing's overall rating (out of 100) | \n",
    "\n",
    "The latter contains the following features:\n",
    "\n",
    "| Variable   | Description |\n",
    "|------------|-------------|\n",
    "| `listing_id` | ID of the listing in Airbnb |\n",
    "| `review_id`  | Id of the review |\n",
    "| `date`       | Date when the review was posted |\n",
    "\n",
    "\n",
    "### Data Preparation\n",
    "In this opportunity, we are in charge of preparing the data for the coming data analysis stages. To do so, we have identified some requirements defined as follows.\n",
    "\n",
    "| Requirement | Subprocess | Aspect | Requirement |\n",
    "|-------------|:-----------|:-------|:------------|\n",
    "| R1 | Data access | -      | Access the `listings.csv` file |\n",
    "| R2 | Data access | -      | Access the `reviews.csv` file  |\n",
    "| R3 | Data access | -      | Represent the listings dataset as a dictionary of dictionaries |\n",
    "| R4 | Data transformation  | Data enrichment | Count the number of reviews of a listing |\n",
    "| R5 | Data integration | - | Add the number of reviews variable to the listings dataset |\n",
    "| R6 | Data cleanse | Erroneous/Noisy data  | Remove non-reviewed listings |\n",
    "| R7 | Data cleanse | Missing values        | Replace empty rating values by -1 |\n",
    "| R8 | Data cleanse | Data format/type)     | Change the data types of the `accommodates`, `price`, and `review_scores_rating` to numeric |\n",
    "| R9 | Data transformation | Data reduction | Remove the `district` variable |\n",
    "| R10 | Data store  | -    | Store the prepared dataset | \n",
    "\n",
    "Hereafter, we will use Python to address each of these requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R1. Access the `listings.csv` file\n",
    "\n",
    "We open the `listings.csv` file and we print its header and the first two rows to see its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/listings.csv') as listings_file:\n",
    "    for i in range(0, 3):  # Print header and first two rows\n",
    "        print(listings_file.readline().rstrip()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R2. Access the `reviews.csv` file\n",
    "\n",
    "As we did before, we open now the `reviews.csv` file and we print its header and the first two rows to see its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/reviews.csv') as ratings_file:\n",
    "    for i in range(0, 3):  # Print header and first two rows\n",
    "        print(ratings_file.readline().rstrip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R3. Represent the listings dataset as a dictionary of dictionaries\n",
    "\n",
    "After reading the CVS file we will transform it into a dictionary of dictionaries. \n",
    "The keys of the parent dictionary will be the listing IDs. \n",
    "Regarding the keys of the nested dictionaries, they will be named as the remaining variables of the dataset. \n",
    "The file has some strange characters that cannot be read properly, that is why we have added the `encoding='utf8', errors='ignore'` arguments to the `open` function; all erroneous characters will be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from typing import Dict, List\n",
    "\n",
    "def access_data(path_listings: str) -> Dict[str, Dict[str, any]]:\n",
    "    \"\"\" \n",
    "    Accesses and returns the listings dataset as a dictionary of \n",
    "    dictionaries. Keys are listing_ids.\n",
    "    :param path_listings: path to the listings dataset\n",
    "    :returns: a dictionary whose strings are listings IDs and its values\n",
    "        a dictionary of properties.\n",
    "    \"\"\"\n",
    "    listings: Dict = dict()\n",
    "    \n",
    "    with open(path_listings, encoding='utf8', errors='ignore') as file: # Ignore erroneous characters\n",
    "        reader = csv.DictReader(file)\n",
    "        \n",
    "        for row in reader:\n",
    "            listing_key: str = row['listing_id']  # Get listing ID\n",
    "            listing_val: Dict = dict()            # Create nested dictionary\n",
    "            \n",
    "            for key in row.keys():\n",
    "                if key != 'listing_id':           # For all variables in the CSV file we will\n",
    "                    listing_val[key] = row[key]   # create an item in the nested dictionary\n",
    "                    \n",
    "            listings[listing_key] = listing_val   # Add the nested dict to the parent dict\n",
    "    \n",
    "    return listings\n",
    "\n",
    "data = access_data('datasets/listings.csv')\n",
    "data['281420']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R4. Count the number of reviews of a listing\n",
    "\n",
    "We want to aggregate the data of the reviews–that is, we want to count the total number of reviews for each listing. \n",
    "For that, we create the function `agggregate_reviews` with takes the `reviews.csv` path as a parameter and returns a dictionary where the key represents the listing ID and the value represents the number of reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from typing import Dict, List\n",
    "\n",
    "def agggregate_reviews(path: str) -> Dict['str', int]:\n",
    "    \"\"\" \n",
    "    Returns a dictionary where listing IDs are mapped to the number \n",
    "    of reviews in the given file.\n",
    "    :param path: path to the reviews dataset\n",
    "    :returns: dictionary with listing IDs as keys and number of reviews\n",
    "        as values.\n",
    "    \"\"\"\n",
    "    reviews: Dict = dict()\n",
    "    \n",
    "    with open(path) as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        \n",
    "        for row in reader:\n",
    "            listing_id: str = row['listing_id']      # Get listing ID\n",
    "            count: int = reviews.get(listing_id, 0)  # Get previous count or initialize it to 0\n",
    "            reviews[listing_id] = count + 1          # Add one to the count\n",
    "            \n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R5. Add the number of reviews variable to the listings dataset\n",
    "\n",
    "We will enrich the listings dataset with the aggregated reviews–that is, we will add a new key (a.k.a. `reviews`) to the nested dictionaries. \n",
    "This new item will display the number of reviews associated with the listing.\n",
    "\n",
    "**We will implement this requirement together with the next one!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R6. Remove non-reviewed listings\n",
    "\n",
    "We only care about listings with reviews. The other observations will be excluded in the data analysis stages. Thus, we need to remove all these cases from the data.\n",
    "\n",
    "**To favor performance, we will comply with both requirements (R5 and R6) in the following code cell!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def access_data(path_listings: str, path_reviews: str) -> Dict[str, Dict[str, any]]:\n",
    "    \"\"\" \n",
    "    Accesses and returns the listings dataset as a dictionary of \n",
    "    dictionaries. Keys are listing_ids.\n",
    "    :param path_listings: path to the listings dataset\n",
    "    :param path_reviews: path to the reviews dataset\n",
    "    :returns: a dictionary whose keys are listing IDs and its values\n",
    "        a dictionary of properties.\n",
    "    \"\"\"\n",
    "    listings: Dict = dict()\n",
    "    \n",
    "    with open(path_listings, encoding='utf8', errors='ignore') as file:\n",
    "        reviews: Dict = agggregate_reviews(path_reviews)  # Get the reviews variable\n",
    "        reader = csv.DictReader(file)\n",
    "        \n",
    "        for row in reader:\n",
    "            listing_key: str = row['listing_id']  # Get listing ID\n",
    "            listing_val: Dict = dict()            # Create nested dictionary\n",
    "            \n",
    "            for key in row.keys():\n",
    "                if key != 'listing_id':          # For all variables in the CSV file we will \n",
    "                    listing_val[key] = row[key]  # create an item in the nested dictionary\n",
    "            \n",
    "            listing_val['reviews'] = reviews.get(listing_key, 0)  # Integrate data: add reviews variable\n",
    "            listings[listing_key] = listing_val                   # Add the nested dict to the parent dict\n",
    "    \n",
    "    return listings\n",
    "\n",
    "data = access_data('datasets/listings.csv', 'datasets/reviews.csv')\n",
    "data['281420']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R7. Replace empty rating values by -1\n",
    "\n",
    "Given a nested listing dictionary, we aim at replacing all empty rating values with `-1`. \n",
    "For that purpose, we define the function `replace_empty_rating`, which takes a nested listing dictionary and modifies it accordingly. \n",
    "This function will be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_empty_rating(listing: Dict[str, any]) -> Dict[str, any]:\n",
    "    \"\"\" \n",
    "    Replaces all empty rating values by -1.\n",
    "    :param listing: dictionary representign a listing\n",
    "    :returns: the input dictionary with empty ratings replaced with -1.\n",
    "    \"\"\"\n",
    "    rating: str = listing['review_scores_rating']  # Get rating value\n",
    "        \n",
    "    if rating == '':                               # If empty\n",
    "        listing['review_scores_rating'] = '-1'     # assign the '-1' value\n",
    "        \n",
    "    return listing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R8. Change the data types of the `accommodates`, `price`, and `review_scores_rating` to numeric\n",
    "\n",
    "Now, we would like to modify the types of the `accommodates`, `price`, and `review_scores_rating` to integer values. Thus, we define the function `change_data_types`, which takes a nested listing dictionary and modifies it accordingly. \n",
    "This function will be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_data_types(listing: Dict[str, any]) -> Dict[str, any]:\n",
    "    \"\"\" \n",
    "    Changes the types of the accommodates, price, and \n",
    "    review_scores_rating to integer.\n",
    "    :param listing: listing dictionary\n",
    "    :returns: listing dictionary with modified types.\n",
    "    \"\"\"\n",
    "    listing['accommodates'] = int(listing['accommodates'])\n",
    "    listing['price'] = int(listing['price'])\n",
    "    listing['review_scores_rating'] = int(listing['review_scores_rating'])\n",
    "        \n",
    "    return listing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R9. Remove the district variable\n",
    "\n",
    "Lastly, we have seen that the `district` variable is error-prone–it has many empty values. \n",
    "We do not consider that this variable is relevant for our analysis. \n",
    "Therefore, we will remove it from the nested dictionaries. \n",
    "We define then the `remove_district_var`, which modifies a nested listing accordingly. \n",
    "The function will be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_district_var(listing: Dict[str, any]) -> Dict[str, any]:\n",
    "    \"\"\" \n",
    "    Removes the district item from the dictionary.\n",
    "    :param listing: removes the district variable from the listing\n",
    "        dictionary\n",
    "    :returns: the listing dictionary without the district variable.\n",
    "    \"\"\"\n",
    "    del listing['district'] # Remove the district key-value from the dict\n",
    "    return listing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply R7, R8, and R9\n",
    "\n",
    "We will now use the functions defined previously to clean and transform our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data: Dict[str, Dict[str, any]]) -> Dict[str, Dict[str, any]]:\n",
    "    \"\"\" \n",
    "    Creates, cleans, and transforms the Airbnb dataset.\n",
    "    :param data:\n",
    "    :returns:\n",
    "    \"\"\"\n",
    "    clean_data: Dict = dict()\n",
    "    \n",
    "    for listing_id in data:\n",
    "        listing: str = data[listing_id]\n",
    "        reviews: int = listing['reviews']\n",
    "        \n",
    "        if reviews == 0:                                       # Data cleanse: Remove non-reviewed listings\n",
    "            continue\n",
    "        \n",
    "        clean_data[listing_id] = listing\n",
    "        clean_data[listing_id] = replace_empty_rating(listing) # Data cleanse: Replace empty ratings by -1\n",
    "        clean_data[listing_id] = change_data_types(listing)    # Data cleanse: Change data types\n",
    "        clean_data[listing_id] = remove_district_var(listing)  # Data transformation: Remove district variable\n",
    "       \n",
    "    return clean_data\n",
    "\n",
    "dataset = create_dataset(data)\n",
    "dataset['281420']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R10. Store the prepared dataset\n",
    "\n",
    "Finally, we will store the resulting dataset in a new CSV file, so we avoid performing the same cleaning every time we need to analyze the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_data(data: Dict[str, Dict[str, any]], path: str) -> None:\n",
    "    \"\"\" \n",
    "    Stores the dataset in the file given as a parameter.\n",
    "    :param data: dataset to store\n",
    "    :param path: path of the output file\n",
    "    \"\"\"\n",
    "    with open(path, 'w') as outfile:\n",
    "        varnames = [\n",
    "            'name',\n",
    "            'neighbourhood',\n",
    "            'city',\n",
    "            'property_type',\n",
    "            'room_type',\n",
    "            'accommodates',\n",
    "            'amenities',\n",
    "            'price',\n",
    "            'review_scores_rating',\n",
    "            'reviews'\n",
    "        ]\n",
    "        writer = csv.DictWriter(outfile, fieldnames=varnames)\n",
    "        writer.writeheader()\n",
    "        \n",
    "        for key in data:\n",
    "            writer.writerow(data[key])\n",
    "            \n",
    "store_data(dataset, 'datasets/out_listings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We made it! The data is ready to be used for the exploratory data analysis stage!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "This Jupyter Notebook is based on Chapter 3 of the book \"Data Preparation for Data Mining\" by Dorian Pyle.\n",
    "\n",
    "Other relevant references:\n",
    "* Kwak, S. K., & Kim, J. H. (2017). *Statistical Data Preparation: Management of Missing Values and Outliers*. Korean journal of anesthesiology, 70(4), 407–411. DOI: 10.4097/kjae.2017.70.4.407. Find it [here](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5548942/pdf/kjae-70-407.pdf).\n",
    "* Saluja, C. (2018). *Data Preparation — A Crucial Step in Data Mining*. Medium. Find it [here](https://medium.com/@chhavi.saluja1401/data-preparation-a-crucial-step-in-data-mining-dba35772f281).\n",
    "* Tachepun, C. and Thammaboosadee, S. (2020). *A Data Masking Guideline for Optimizing Insights and Privacy Under GDPR Compliance*. In the 11th International Conference on Advances in Information Technology. ACM, New York, Article 22, 1–9. DOI: 10.1145/3406601.3406627. Find it [here](https://dl.acm.org/doi/abs/10.1145/3406601.3406627)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# (End of Notebook)\n",
    "\n",
    "&copy; 2021-2022 - **TU/e** - Eindhoven University of Technology"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
